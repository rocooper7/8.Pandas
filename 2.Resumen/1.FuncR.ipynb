{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- #\n",
    "# Import exploration files \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "file_path = 'C:path\\\\to\\\\data\\\\file_name{}'\n",
    "Ctrl + f ('buscar')\n",
    "axis = 0 (rows) - axis = 1 (columns) \n",
    "######################################################################################################################################### \n",
    "'Data Read & Write'\n",
    "######################################################################################################################################### \n",
    "# Read in data \n",
    "df = pd.read_csv(file_path.format('.csv'))  #encoding='utf-8'   (header=2)  #header para quitar las filas que no nos sirven de arriba del df \n",
    "df = pd.read_excel(file_path.format('.xlsx'))  \n",
    "df = pd.read_json(file_path.format('.json'))\n",
    "df = pd.read_pickle(file_path.format('.pkl'))\n",
    "df = pd.read_parquet(dir_pandas.format('.parquet'))\n",
    "df = pd.read_hdf(dir_pandas.format('.hdf'))\n",
    "# Write to  \n",
    "df.to_csv('data_out.csv',index=False)  #Sin escribirle una ruta \n",
    "\n",
    "df.to_csv(file_path.format('.csv'), index = False)      #Mandandolo a una ruta en específico \n",
    "df.to_excel(file_path.format('.xlsx'), index=False, sheet_name='Hoja_uno')\n",
    "df.to_json(file_path.format('.json'))\n",
    "df.to_pickle(file_path.format('test.pkl'))\n",
    "df.to_parquet(file_path.format('test.parquet'))   #Archivos para Big Data\n",
    "df.to_hdf(file_path.format('test.hdf'),key='data',format='table')   #Big Data Hadoop\n",
    "\n",
    "# Conexiones Bases de Datos SQL \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# MySQL\n",
    "engine = create_engine('mysql+pymysql://root:Cooperboy0071985@localhost/mysql7', echo=False)  \n",
    "conn = engine.connect()\n",
    "sql = 'select * from sample_database'\n",
    "df = pd.read_sql(sql, conn)\n",
    "df.to_sql('File_name', con=engine, if_exists='append', index=False, chunksize=50) \n",
    "# SQLServer\n",
    "engine = create_engine('mssql://sa:Cooperboy0071985@DESKTOP-GBM1V5B/Northwind?driver=SQL Server Native Client 11.0') \n",
    "conn = engine.connect()\n",
    "sql = 'select * from Orders'\n",
    "df = pd.read_sql(sql, conn)\n",
    "df.to_sql('File_name', con=engine, if_exists='append', index=False, chunksize=50)  \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Exploration'\n",
    "#########################################################################################################################################\n",
    "# Para que mi df y analisis aparezcan con x num de decimales para pandas \n",
    "pd.options.display.float_format = '{:,.1f}'.format      #Pandas\n",
    "pd.set_option('display.float_format', '{:,.1f}'.format)  #Otra forma Pandas\n",
    "np.set_printoptions(precision=1)         #Numpy \n",
    "\n",
    "# Rows and columns returns (rows(0), columns(1))\n",
    "df.shape    \n",
    "\n",
    "# Returns the first x number of rows when head(num). Without a number it returns 5\n",
    "df.head()\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = 10\n",
    "display(df)\n",
    "\n",
    "# Returns the last x number of rows when tail(num). Without a number it returns 5\n",
    "df.tail()\n",
    "\n",
    "# Muestra de algunas filas de un df\n",
    "df.sample()\n",
    "\n",
    "# Returns an object with all of the index or columns values \n",
    "df.index #.values\n",
    "df.columns #.values  #list(df)\n",
    "df['C1'].values #Array )Puede ser a una columna o todo el df \n",
    "\n",
    "# Unique values of each column\n",
    "df.nunique()  #Todo el df \n",
    "df.col_name.unique()  # nunique valores #Una columna etiquetas\n",
    "\n",
    "# Cuenta de valores\n",
    "df['C1'].value_counts()\n",
    "df['C1'].value_counts() / len(df['C1']) * 100  #Sacar el % de los valores \n",
    "for col_name in categoricas:   #Se crea una lista de los nom de las columnas categoricas primero\n",
    "    print(\"Distribución de \",col_name,\": \\n\", 100* df[col_name].value_counts()/df.shape[0],\"\\n\")  #Ver toda la cuenta de las cols\n",
    "\n",
    "# Basic information on all columns \n",
    "df.info()\n",
    "\n",
    "# Gives basic statistics on numeric columns\n",
    "df.describe()   #Valores Cuantitativos\n",
    "df.describe(include='all')   #Todos los valores del df\n",
    "\n",
    "# Shows what type the data was read in as (float, int, string, bool, etc.)\n",
    "df.dtypes\n",
    "df.convert_dtypes().dtypes   #Convertir los datos, a sus datos originales object por string\n",
    "types = pd.DataFrame(df.dtypes)   # Ver Agg de tipo de dato por variable o utilizar df.nunique()\n",
    "print(\"Tipos de variables\",types.groupby(0).size())\n",
    "types.index[types[0] == 'object'].values  #Para ver nom de cols de tipo de dato (objeto,int64,bool) o se puede ver tambien abajo con select_dtypes\n",
    "\n",
    "# Renombrar tipos de datos\n",
    "df['C1'].astype(float)    #int == 'int64'\n",
    "df[['C1','C2']] = df[['C1','C2']].astype('category') #Float, Int etc \n",
    "df['C1'] = pd.Categorical(df['C1'].apply(str))  #Otra forma de asignar una variable ctegorica \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Null'\n",
    "#########################################################################################################################################\n",
    "#Valores Nulos \n",
    "np.nan  #nulo numpy\n",
    "pd.options.mode.use_inf_as_na = True  #Activar nulos de pandas \n",
    "pd.NA #Nulo Pandas \n",
    "\n",
    "# NaN Shows which values are null\n",
    "df.isnull()  #isnull == isna , notnull == notna\n",
    "\n",
    "# Shows which columns have null values\n",
    "df.isnull().any()\n",
    "\n",
    "# Num total de valores nulos por columna y en %\n",
    "df.isnull().sum() / data.shape[0]   #Primera forma\n",
    "df.isna().sum(axis=0)#/len(df)      #Segunda (axis=1 por columnas )   \n",
    "df.notnull().apply(pd.Series.value_counts)  #Tercera /df.shape[0]*100 para ver %\n",
    "(df != 0).apply(pd.Series.value_counts)   #Cuarta\n",
    "((df != 0) & (df.notnull())).col_name.value_counts() #Comprobación cuando se quitan\n",
    "df.size-df.isnull().sum().sum()  #Nulos de todo el df \n",
    "\n",
    "#Filtro Nulos\n",
    "df[df['C1'].notnull()]     #Boolean Filtering\n",
    "\n",
    "#Quitar Nulos\n",
    "df.dropna()  #Ojo en todo el df \n",
    "df[['a']].dropna()  #Por columna   []Serie y [[]] df\n",
    "\n",
    "# Rellenar valores nulos en todo el df\n",
    "df.fillna(0)   \n",
    "df.fillna(method=\"ffill\")    #Rellenar con valores de arriba de la columna \n",
    "df.fillna(method=\"bfill\")      #Rellenando con el valor de abajo de la columna \n",
    "df.fillna(method=\"bfill\",axis=1)      #Rellenando con valores de la derecha \n",
    "df.fillna(df.median())    #df['C1'].fillna(df['C1'].median())  una columna \n",
    "#Rellenando valores nulos en una columna \n",
    "df['C1'] = df['C1'].fillna(df['C1'].median()) \n",
    "df['C1'] = df['C1'].fillna('S')   #Rellenando Strings\n",
    "df = pd.concat([df[['C1']], df[['C1']].interpolate()],axis=1)   #Valores interpolados \n",
    "df.columns = ['antes','interpolado']\n",
    "#Rellenando valores en especifico por filas y columna\n",
    "missing_title_mask = df['title'].isna()   #Se crea una mascara de una columna de valores nulos \n",
    "df_missing_titles = (df[missing_title_mask]['url']     #Se pasa la mask sobre otra col donde se quiere extraer la info  \n",
    "                      .str.extract(r'(?P<missing_titles>[^/]+)$')\n",
    "                      .applymap(lambda title: title.split('-'))\n",
    "                      .applymap(lambda title_word_list:' '.join(title_word_list)))\n",
    "df.loc[missing_title_mask,'title'] = df_missing_titles.loc[:, 'missing_titles']    #Cuando queremos rellenar valores en especifico \n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.impute import SimpleImputer     # Se reemplazan los valores nulos de las columnas, con valores promedio o mediana\n",
    "imputer = SimpleImputer(missing_values= np.nan,  strategy = 'median')\n",
    "\n",
    "values = imputer.fit_transform(df)   #Llenamos los valores nulos con el promedio \n",
    "X = pd.DataFrame(values)       #convertimos nuestros valores en un df \n",
    "X.columns = df.columns    #Y le pasamos los nombres de las columnas \n",
    "X.index = df.index         #Y el indice \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Strings'\n",
    "#########################################################################################################################################\n",
    "df['C1'].str.lower()  #upper,capitalize\n",
    "df['C1'].str.len() \n",
    "df['C1'].str.split(' ') #'-' \n",
    "df['C1'].str[:5]\n",
    "df['C1'].str[-5:]\n",
    "df['C1'].str.replace('Alfredo','Antonio') \n",
    "df['C1'].str.findall('ara')\n",
    "df['C1'].str.contains('or')\n",
    "df['C1'].str.lower().str.count('a')\n",
    "df['C1'].str.extract('([0-9]+)', expand=False)\n",
    "df['C1'].str.replace('@[^\\s]+','')\n",
    "df['C1'].str.slice(start=4).astype(float).astype(int)  #start=4 numero de letra en donde empieza a partir \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Time'\n",
    "#########################################################################################################################################\n",
    "# Tiempo \n",
    "df['C_date']  = pd.to_datetime(df['C_date'], errors = 'coerce', format = '%m/%d/%Y %H:%M:%S %p')\n",
    "df['C_new'] = df['C_date'].dt.hour #Extrayendo un dato de tiempo en especifico (year,month,weekday,quarter,week,day)\n",
    "list_date = [df['C_date'][i].month for i in range(df.shape[0])]   #Otra forma de sacarlo\n",
    "df['C_weekday'] = df['C_date'].apply(lambda x: x.weekday())\n",
    "mask = df['year'] > pd.to_datetime('1970-01-01')         #Boolean filtering\n",
    "df = df[mask]\n",
    "\n",
    "# Series de tiempo  (Groupby)\n",
    "df_t = df.groupby('Date').sum()  \n",
    "#Diferencia por tiempo\n",
    "df_t.diff().mean()   #Me trae el prom por dia \n",
    "df_diff = df_time.diff()   #diff resta cada dia con el dia anterior, para ir viendo el aumento de casos dia a dia   (df modificado)  \n",
    "df_t.head(1).to_dict()   #Sacamos los datos del df_time, como dicc \n",
    "df_diff = df_diff.fillna({'Confirmed': 555.0,'Deaths': 17.0,'Recovered': 28.0})  #Rellenamos los valores nulos de la primera fila que teniamos\n",
    "df_diff.cumsum()  #Se trae el df original agrupado \n",
    "# Agrupación por fechas (Resample)\n",
    "df_t.resample('M').sum()   #'7D'(7dias),'W-Sun' (dia semana (domingo))  #resample me agrupa valores de tiempo determinados como mes (M), dias (7D) etc \n",
    "df_t.resample('M').count() \n",
    "df_t.resample('M').mean()\n",
    "#Variables Nulas \n",
    "df = df_t.resample('12h').sum(min_count=1)\n",
    "df = df.interpolate()   #Se rellenan con valores interpolados\n",
    "\n",
    "#Forma resumida de agrupar \n",
    "df = df.groupby(['C1_G',pd.Grouper(key='C2_GDate',freq='1D')]).sum()   #Todos los valores de AGG \n",
    "df = df.groupby(pd.Grouper(key='C1_GDate', freq = 'M'))[['C2_AGG']].mean()\n",
    "df = df.groupby(['C1_G',pd.Grouper(key='C2_GDate', freq='1Y')]).agg({'C3_AGG':[np.mean,np.median]})   \n",
    "#Visualización 1\n",
    "df = df.groupby(pd.Grouper(key='C1_GDate', freq = '1D'))['rate'].mean()    \n",
    "df.plot()       #Hasta aqui es la grafica \n",
    "df.rolling(window=7).mean().plot()         #7 dias, Extra \n",
    "df.rolling(window=14).mean().plot();  \n",
    "#Visualización 2 \n",
    "df.plot(figsize = (10,7), title = 'CODV-19')\n",
    "plt.xlabel('Date')  \n",
    "plt.ylabel('People')\n",
    "plt.show()\n",
    "#Visualización 4 (mes)  barras \n",
    "df_monthly.plot(figsize = (10,7), kind='bar');\n",
    "#Visualización 5  Barras agrupadas \n",
    "df_monthly.plot(figsize = (10,7), kind='bar', stacked = True)\n",
    "#Visualización 5 Pie \n",
    "df_monthly[['C1', 'C2', 'C3']].T.plot(figsize = (10,7), kind = 'pie', subplots=True);\n",
    "#Visualización 6 \n",
    "df = df.groupby(['C1_G',pd.Grouper(key='C2_GDate', freq='1Y')]).agg({'C3_AGG':[np.mean,np.median]}) \n",
    "df.xs('Value_C1_G')['AverageTemperature'].plot();\n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Manipulation'\n",
    "#########################################################################################################################################\n",
    "#Crear serie \n",
    "s = pd.Series([100, 200, 300])     #,index=['a','b','c']\n",
    "s = pd.Series({1999: 48, 2000: 65, 2001: 89})\n",
    "\n",
    "#Crear Dataframe\n",
    "df = pd.DataFrame({1999: [74, 38, 39], 2000: [34, 32, 32], 2001: [23, 39, 23]})\n",
    "\n",
    "# Copia de un df\n",
    "df2 = df.copy(deep=True) \n",
    "\n",
    "#Rename columns \n",
    "df.rename(index=str columns={'C_old':'C_new'}, inplace= True)  #1ra Forma sencilla \n",
    "df.columns = new_column_names  #2da Forma sencilla  (por listas) \n",
    "\n",
    "#Reordered Columns\n",
    "col_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Date', 'Transportation Expense']\n",
    "df = df[col_names_reordered]\n",
    "\n",
    "# View all rows for one column\n",
    "df.col_name \n",
    "df['C1']\n",
    "\n",
    "# Multiple columns by name\n",
    "df[['C1','C2']]\n",
    "\n",
    "#Accediendo a una subcolumna \n",
    "df = df['C_Sup'][['Col_sub']]\n",
    "\n",
    "# Loc (Rows & Columns by Label)\n",
    "df.loc[0:2,['C1','C2']]\n",
    "df.loc[df['C1']>13]#['C2']\n",
    "df.loc[df.col_name > 20,['C1','C2','C3']]\n",
    "df.loc[(df.col_name1 > 21) & (df.col_name2 == 10),'nombre']  #Lógica x nombre\n",
    "\n",
    "# Iloc (Rows & Columns by index) \n",
    "df.iloc[:,0:2]\n",
    "df['C1'].iloc[::2]        #Separa filas de 2 en dos \n",
    "df['C1'].iloc[::2] - df['C2']     #Resta los pares y los demas los pone en NAN   \n",
    "df.iloc[:, 1:14].sum(axis=1)    #sum  \n",
    "\n",
    "# Sort (Ordenar valores) \n",
    "df.sort_index(axis = 0, ascending=False)\n",
    "df.sort_values('col_name',ascending=False)   #by= 'col_name'  (ascending = [True,True])\n",
    "\n",
    "# Drop columns \n",
    "df.drop(['C1'], axis =1) #add inplace = True to do save over current dataframe\n",
    "df.drop([0,2,4,6]).head(5)       #Quitar filas \n",
    "# Drop multiple \n",
    "df.drop(['C1','C2'], axis =1)\n",
    "#Delete columns\n",
    "del(df['C1'])\n",
    "\n",
    "# Duplicated (Duplicados)\n",
    "df.duplicated().value_counts()   #De todo el df \n",
    "df[~ df.duplicated()]    #Filtro de los que no son duplicados \n",
    "df['C1'].duplicated().value_counts()   #por columna \n",
    "df.drop_duplicates() #Todo el df \n",
    "df.drop_duplicates(subset=['C1'], keep = 'first', inplace=True)   #keep = last, false, true, quita valores por col \n",
    "df.drop_duplicates(['C1'],keep='first')\n",
    "\n",
    "# Reemplazar valores en un columna \n",
    "df['C1'].replace(1,'A')    # False: 0, True: 1\n",
    "df['C1'].replace({1:'A', 2: 'B',3:'C'})\n",
    "df['C_new'] = df['C1'].map({1:0, 2:1, 3:1, 4:1})\n",
    "\n",
    "# Factorizar \n",
    "paises = ['MX','US', 'BR','AR','CO', 'CA']\n",
    "df['Paises'] = [paises[np.random.randint(0,len(paises))] for i in range(30)]   #Aleatorio de paises  #shape[0]\n",
    "df['idPaises'] = pd.factorize(df['Paises'])[0]  # Categoriza los paises aleatorios\n",
    "\n",
    "# Filtros\n",
    "df(df['edad'] >= 12) & (df['pais'].isin(['mx','ch'])) #Usar Boolean filtering \n",
    "df.query('edad >= 12 & pais == [\"mx\",\"ch\"]')\n",
    "df[df['C1'] >= df['C2']]        #Filtros entre columnas \n",
    "df[(df.Survived == 1)].Sex.value_counts()#[1]  \n",
    "\n",
    "# Lambda function \n",
    "df.apply(lambda x: x.col_name**2, axis =1)   #Se especifica dentro de lambda la columna \n",
    "df.col_name.apply(lambda x: x**2) #Se aplica directamente sobre la columna\n",
    "'Yes' if fruit == 'Apple' else 'No'\n",
    "\n",
    "# Lambda function Strings\n",
    "df.apply(lambda tokens: list(map(lambda token: token.lower(), tokens)))\n",
    "df.apply(lambda word_list: list(filter(lambda word: word not in stop_words, word_list)))\n",
    "df.applymap(lambda title: title.split('-'))\n",
    "\n",
    "#Selección de df de variables Cat, Num, Obj  #Metodo 1\n",
    "df.select_dtypes(exclude='number').columns.values # Selecciona todas las columnas que no sean tipo numerico ( int | float )\n",
    "df.select_dtypes(include='object') # Solo columnas tipo objeto\n",
    "df.select_dtypes(include=['int64', 'categorical', 'date'])\n",
    "\n",
    "#Selección de df de variables Cat, Num, Obj  #Metodo 2\n",
    "df2 = df1.drop(['C1', 'C2', 'C3', 'C4', 'C5'], axis = 1)   #Quitamos Col inecesarias e identificadores\n",
    "cat_cols = [c for c in df2.columns if df2[c].nunique() < 10 and df2[c].dtype == 'object']\n",
    "num_cols = [c for c in df2.columns if df2[c].dtype in ['int64', 'float64']]\n",
    "my_cols = cat_cols + num_cols   #Opcional\n",
    "df2 = df2[my_cols]\n",
    "df3 = pd.get_dummies(df2)    #Se le pueden pasar el df depurado para sacar un df de dummies\n",
    "\n",
    "# Convertir variables categoricas en dummies pandas  \n",
    "df = pd.get_dummies(df['C1'])  #Se puede con una columna o todo df de var cat, #Se usa concat (df original + df dummies)   ,drop_first = True)\n",
    "old_col = df.columns   #Cambio iterativo de nombres de columns (OneHotEncoding), no es necesario (se puede solo con get dummies)\n",
    "new_columns = ['C1'+ '_' + str(i) for i in old_col]  \n",
    "columns_uid = {old:new for old,new in zip(old_col,new_columns)}\n",
    "df.rename(index=str, columns=columns_uid, inplace= True)\n",
    "\n",
    "df_meteorites[['C_new1','C_new2']] = pd.get_dummies(df_meteorites['C1'])  #Para guardarlo directamente en el df con dos columnas \n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  #Transformamos una columna en 0 y 1\n",
    "label_encoder = LabelEncoder()\n",
    "encoder_C1 = label_encoder.fit_transform(df['C1']).reshape(-1,1)  #Cuando solo hay dos resultados, hom, muj. Si hay más (.reshape(-1,1)) \n",
    "#encoder_C1 [:10]  #Si son mas resultados se pasa a lo siguiente\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "column = ['C1' + \"_\"+ str(i) for i in label_encoder.classes_]   #Se crean los titulos de las columnas\n",
    "pd.DataFrame(encoder.fit_transform(encoder_C1),columns = column).head(10)  #Ver Redes Neuronales (5_Clasificacion Binaria)\n",
    "\n",
    "#Cut (Partir una columna en segmentos)\n",
    "pd.cut(df['C1'],bins = 3).value_counts()\n",
    "df['C_new'] = pd.cut(df['C1'],bins = [3,18,35,60]) \n",
    "\n",
    "#Transponer df\n",
    "df.T\n",
    "\n",
    "#Reset Index\n",
    "df.reset_index()  \n",
    "df.reset_index(drop=True)  #Quitamos el indice \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Union'\n",
    "#########################################################################################################################################\n",
    "# Concat (agregar tabla a otra tabla, (axis= 0 (filas,vertical) o axis=1 (columnas,horizontal)\n",
    "# Series\n",
    "pd.concat([s1,s2], axis=1)\n",
    "# Df\n",
    "pd.concat([df1,df2], axis=1)\n",
    "pd.concat([df1,df2], axis=1, join='inner')  #Join con concat\n",
    "\n",
    "# Merge (JOIN in SQL)\n",
    "pd.merge(df1, df2, how = 'inner' , on = 'C_key')      #left,right,outer   (df1.merge(df2,.....))\n",
    "df = pd.merge(df[['C1','C2','C3']],df2, on = ['C_key1','C_key2'], how = 'inner')  #Df1 columnas que quiero y dos llaves valor unico \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Aggregation'\n",
    "#########################################################################################################################################\n",
    "#Multiples indices \n",
    "df = df.set_index(['C1','C2']).sort_index()\n",
    "df.index.get_level_values(0)   #Obtener valores del indice (0) primer idx\n",
    "df.unstack('year')   #como tipo pivot por idx\n",
    "#Localizacion multiples indices\n",
    "df.loc['C1',:].loc['C2',:]\n",
    "df.xs(['C1','C2']) \n",
    "df.xs('2018', level='year')   #level es por idx \n",
    "ids = pd.IndexSlice\n",
    "df_countries.loc[ids['Aruba':'Austria','2015':'2017'],:].sort_index()\n",
    "df_countries['pob']['Colombia']['2018']\n",
    "#Operacion mult idx\n",
    "df.sum(level='idx1')  #o indice 2 \n",
    "\n",
    "#Groupby  (#C1_G Col agrupada, C2_AGG Col Agregaciones(sum,mean,count,max,min))\n",
    "df.groupby('C1_G').mean()   #Col agrup y todas las Cols Agg (df)  \n",
    "df.groupby('C1_G')['C2_AGG'].sum() #Por Col agrup y Col Agg\n",
    "df.groupby(['C1_G','C2_G'])[['C3_AGG']].sum()  #Mult agrupaciones (idx)\n",
    "df.groupby('C1_G').C2_AGG.value_counts()\n",
    "df.groupby('C1_G')[['C2_AGG','C3_AGG']].describe()  # 2Agg\n",
    "df.groupby(['C1_G','C2_G'])[['C3_AGG','C4_AGG']].apply(np.std)\n",
    "#Extraer valor de 1s\n",
    "df.groupby(['C1_G','C2_G']).count()   #Se saca la cuenta de forma diferente\n",
    "df['ones'] = 1    #Se asigna una columna de 1s \n",
    "df2= df.groupby(['C1_G','C2_G'])[['ones']].sum()\n",
    "df2.groupby(level=0).apply(lambda x:x / x.sum() * 100)  #Para sacar el porcentaje de los valores  \n",
    "#To_Frame    \n",
    "df.groupby('C1_G')['C2_AGG'].sum().to_frame()  #o [['C1']] (Se convierte en df)\n",
    "#Aggregate    \n",
    "df.groupby(['C1_G','C2_G'])['C3_AGG'].agg(['min',np.mean,max])  #Mult Agrup y Mult Agg para una Col Agg\n",
    "df.groupby(['C1_G','C2_G'])['C3_AGG'].agg([min,np.mean,max,lambda x: np.mean(x)/1000])\n",
    "dict_agg = {'carat':[min,max], 'price':[np.mean]}\n",
    "df.groupby(['C1_G','C2_G']).agg(dict_agg)\n",
    "#Filter (Having SQL)\n",
    "df.groupby('C1_G').filter(lambda x: x['C2_AGG'].sum() > 10)\n",
    "df.groupby(['C1_G','C2_G']).filter(lambda x: (np.mean(x['C3_AGG'])/1000)>6)\n",
    "df.groupby(['C1_G','C2_G']).filter(lambda x: (np.mean(x['C3_AGG'])/1000)>6)['C1_G'].unique()  #Comprobación \n",
    "df.groupby('C1_G')[['C2_AGG','C3_AGG']].apply(lambda x: np.mean(x) * 1.12)  \n",
    "#Para sacar info por medio de un for \n",
    "for C1_G,group in df.groupby('C1_G')['C2_AGG']:\n",
    "  grouped_sized = group.mean()\n",
    "  print(f'Name1: {cut}, Name2: {grouped_sized} \\n')\n",
    "\n",
    "# Pivot table (Tablas dinámicas)\n",
    "df = df.pivot_table(values='C1', index='C2', columns='C3')\n",
    "df.pivot_table(index = 'C1', values = 'C2', columns = 'C3',aggfunc=[np.sum,np.mean]) #fill_value=0\n",
    "df.pivot_table(values='C1', index=['C2',pd.Grouper(key='C3_Date', freq= '1Y')],aggfunc={'C1':[np.mean,np.median]}) #No es necesario values=, \n",
    "#porque en aggfunc, ya se esta agregando en forma de dicc los valores \n",
    "df.unstack().reset_index()    #Para desagrupar y volver a la tabla pivot, en un df normalizado\n",
    "\n",
    "# Melt (Normalización de Tablas)\n",
    "df.loc[:,cols].melt(id_vars=['C1','C2']).rename(columns={'variable':'Date','C1':'C1a','C2':'C2a','value':'Agriculture'})\n",
    "list(map(str,range(1971,2015)))  #Convertir un rango de num a una lista de strins\n",
    "cols = ['C1','C2'] + list(map(str,range(1971,2015)))   #Se utiliza en el melt \n",
    "#########################################################################################################################################\n",
    "'Data Operations'\n",
    "#########################################################################################################################################\n",
    "+ 'add()', - 'sub(), subtract()', * 'mul(), multiply()', / 'truediv(), div(), divide()', // 'floordiv()', % 'mod()', ** 'pow()'\n",
    "\n",
    "# Operaciones aplicables a una columna \n",
    "df['C1']**2 + 10\n",
    "df['C1'].min()  #max()  \n",
    "np.sin(df['C1']**2) + 10\n",
    "\n",
    "# Suma total del df \n",
    "df['C_new'] = df_num.sum(axis=1)     #Suma de un df solo numerico \n",
    "df['C_new'] = df.iloc[:,10:15].sum(axis=1)    #Suma por filas exclusivas iloc o loc \n",
    "df['C_new'].sum(axis=0)   #Suma toda una columna \n",
    "\n",
    "# Operaciones entre dos columnas \n",
    "df['C1'] - df['C2']\n",
    "df['t1'] / df['t2']\n",
    "\n",
    "# Operaciones con Funciones Normales\n",
    "def fun_1(x):      #Se crea una funcion \n",
    "  y = x**2 + 1\n",
    "  return y\n",
    "df['C1'].apply(fun_1)\n",
    "\n",
    "# Operaciones con Funcion Lambda\n",
    "df.apply(lambda x: x.col_name**2 + 1, axis =1)   #Se especifica dentro de lambda la columna \n",
    "df.col_name.apply(lambda x: x**2 + 1) #Se aplica directamente sobre la columna\n",
    "a,b=20,-100\n",
    "df.col_name.apply(lambda x: x**2 + a*x + b) \n",
    "df['C1'].apply(lambda x: x+273)\n",
    "df.apply(lambda x: x.mean())     #Promedio de todo el df en columnas (axis=1 en filas)\n",
    "df.apply(lambda x: x['C1']-x['C2'], axis=1)\n",
    "df.applymap(lambda x: x/1000)   #Aplicar una opearcion a todo el df \n",
    "\n",
    "#########################################################################################################################################\n",
    "'Data Visualization'\n",
    "#########################################################################################################################################\n",
    "# Plot histograms for all numeric columns \n",
    "df['C1'].hist() #figsize = (10,7), bins = 10\n",
    "#Matplotlib hist\n",
    "plt.title('Name')      #Matplotlib\n",
    "plt.hist(df['C1'], edgecolor='black', linewidth=1);\n",
    "\n",
    "#Boxplot Pandas\n",
    "df.boxplot();  #A todo el df \n",
    "# Boxplot pandas por columnas \n",
    "num_features = df[['C1', 'C2', 'C4', 'C5']]\n",
    "plt.figure(figsize=(10,7))\n",
    "num_features.boxplot(grid=False)\n",
    "# Boxplot Seaborn\n",
    "import seaborn as sns\n",
    "sns.boxplot(df.C1, df.C2, hue = df.C3)\n",
    "# Matplotlib \n",
    "plt.boxplot(df['C1'],vert=False);  #Vert= vertical u horizontal"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitanacondaconda74954a3f106b47d99d44e5fb8ca66932",
   "display_name": "Python 3.7.4 64-bit ('Anaconda': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}